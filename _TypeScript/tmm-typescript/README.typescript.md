
### npm run start
#### serve data 8200/api/schools
#### client 8042
https://github.com/TonyCurtisLives/VirtualDog.git
npm install
npm run bower-typings 

npm install jasmine angular-mocks --save-dev
npm run typings -- install dt~jasmine --global --save  // that's a dash-dash before install!
npm run typings -- install dt~angular-mocks--global --save-dev  // that's a dash-dash before install!
 
##

### Use the **feature/begin** and **feature/end** branches to follow along with the [Pluralsight course here](https://www.pluralsight.com/courses/javascript-jasmine-typescript) 

To get started go to your fork directory in a command prompt and do this:

```
npm install
npm start
```

Uses **npm scripts to run locally installed packages** without the need to run from command line, so **everything is local to avoid versioning issues** with a plethora of globally installed versions on student’s ‘puters. 
Since you want to use the local version of stuff (e.g. typescript, typings, etc), you **don’t want to run command line directly (contrary to the course instructions)** since that will either:
- Fail if you don’t have the node module installed globally, 
- Or if you do have it installed globally, it will run the global version which may be different than the local version used in this project.

Instead do this (as needed):
```
npm run tsc 
npm run tsc -- -w
npm run typings -- install dt~jasmine --global --save
```

The pertinent part being `npm run` and if you want to add command line parameters follow the module name with the double dash (`--`) then add your command line parameters.
Right now only bower, tsc, and typings have scripts in the package.json file that will allow this, so if there are other command-line-ish things you want to add to package.json, then add them to the list of scripts
 
Requires the following global installations:

- npm

Also globally installed:

- tslint (used by VSCode) 

### To bring up the Virtual Dog Blog in the browser simply navigate to localhost:8042

### To bring up the Jasmine tests in the browser simply navigate to the test/SpecRunner.html file in your browser

No frills, super simple, just a dog and his blog.
Angular application using Jasmine, a fantastic framework for writing solid unit tests. TypeScript will transport us into the future of JavaScript, transforming our authoring experience, and allowing us to use current and proposed ECMAScript language enhancements. In this course we'll start with the motivations for unit testing, and why crafting good unit tests is so important. We'll look at basic test definition, including test set up, nested scoping, and assertions. We'll discuss dynamic mocking, and the ins and outs of testing with dependencies. We'll look at techniques for creating reliable tests, and some of the consequences when you don't. You don't need to be familiar with Jasmine or TypeScript to dive into this course. Just a general knowledge of JavaScript will do. We'll be using VS code in the course to write our code, but you can use any IDE that's compatible with TypeScript to take advantage of superior tooling support. Along the way you'll meet my dog Zeus, catch some plastic mice, go to mars, and meet some evil twins. What could be better?

Course Introduction and Project Setup
Module Overview
Hi. Welcome to Testing JavaScript with Jasmine and TypeScript. I'm Tony Curtis. In this course you'll learn how to write fantastic unit tests in Jasmine, and how to use TypeScript to turbocharge your testing productivity. In this module we'll talk about why the heck you would ever want to write a unit test, and what to pay attention to as you write them. We'll take a look at Jasmine's easy to use interface for creating solid test code, and Jasmine's easy to read test results. We'll see how TypeScript can inject a super boost of productivity into our testing efforts, and we'll get you set up to write your own tests on the virtual dog project.

Why Write Unit Tests?
You or your boss or your boss's boss may be wondering why you should spend any time writing unit tests, especially when you factor in the cost of maintaining those tests. There are definitely some strong opinions out there, like unit testing takes too much time, too much time to write tests, and too much time to maintain them. Unit tests are useless code. Developers need to spend time enhancing the software, not writing meaningless tests. Unit testing forces you to inject an manage dependencies, and complicates your build process by forcing you to add unit testing and code coverage steps. Let's talk about time, and let's take a developer who uses unit tests. He compiles his code, runs the unit tests, and the test fails, revealing a bug in his code. He fixes the bug, and it's easy because he was writing that code 2 minutes ago. He's totally in the context of the code. He compiles it, and runs the unit test again. Everything passes, and the bug is neatly disposed of. Now let's take a developer who doesn't write unit tests, and let's say a bug in his code makes its way to the user. At first the user is horrified. They can't get their work done. They don't understand what's happening. They talk to some colleagues, maybe the supervisor to see if they're doing something wrong. Unable to resolve the issue, they finally call support. The support person listens carefully to the complaint. If you can get a queen of support on your team that'll help a lot. Support tries to reproduce the problem, calling on all resources necessary to understand the issue. The project team meets to triage the bug, prioritize, estimate the severity, and determine when to schedule the patch release. Developers perform digital forensics on the bug. If it's a cold case they're forced to examine code that hasn't been touched in years. Eventually, they find the bug and fix it. They don't waste any time writing unit tests. Instead, they do some ad hoc testing of the application, confident they have the bug fixed. They request Ops to deploy the code to the test environment, so QA can test it. QA tests the code extensively, since they know the developers don't write any unit tests. Finally, Ops is asked to deploy the change to production, and the users are notified. So here's the question, by not writing unit tests whose time are we saving? The developers? Maybe, until he has to do cold case forensics. What about QA? They love those regression tests. Ops just jumps for joy with each new deployment. How about support? What about the users' time? Are unit tests useless? Seatbelts are useless. They don't make the car run any better. Airbags are useless. They don't add any horsepower. They're all useless until you need them. Then you're glad you have them, but that isn't the whole story. Stringing a safety net also seems like a waste of time. They don't make the acrobats perform any better or do they? With the safety net in place they are free to try new things to refactor their acrobatics. The same is true for software. With that safety net in place product owners, QA, development, everyone involved in the process has more confidence in efforts to refactor, re-architect, and tackle technical debt. Does unit testing add complexity? Actually, it adds clarity. You can read a test result, and know exactly what the class or file is specifically designed to do. Unit testing also favors decoupling. Being able to easily isolate the test subject discourages embedded dependencies. Unit testing encourages simplification. If the testing for a function gets too complex it's likely the function is doing too much. By supporting these solid software principles unit testing also supports extensibility, but ultimately you have to ask yourself this question. Where do you want your dirty laundry? Do you want it to stay in development or do you want to go public with it? Finding bugs with unit tests keeps all of that dirty laundry where it belongs, and saves you and others some awkward moments.

What Could Possibly Go Wrong?
There are definitely some pitfalls to writing unit tests. Let's talk about three of those pitfalls. Brittle tests break whenever changes are made to the code. This is annoying because the test is supposed to signal a failure in the code, not a failure in the test. This is known as a false positive, and we'll talk more about this in the last module. Complicated tests make it difficult for other developers to understand and maintain the test. If the test fails due to a code error or an error in the test the complex test will cost the developer time to understand the test, and determine the cause of the failure. The above two factors contribute to the third, ignored tests. When tests repeatedly fail developers lose confidence in their validity, and mark them as ignored. They do the same with failing tests that are too complex for them to examine quickly. In both of these cases the positive test signal is taken as a false positive and ignored, so how do we make it work? As we write tests we need to apply the solid coding principles to the test code. Don't treat tests like second class citizens. Write them with all the rigor you would give your production code. When we write tests we must learn to test the right things. You can test a lot of things with unit tests, but which of them actually matter? By testing the right things and not over specifying our test subjects we create robust tests. We aren't born with awesome unit testing skills. The only way to learn how to write good unit tests is to write unit tests. Throughout this course we will see examples of good and not so good unit tests. As you learn to spot the goodness you increase your ability to write extraordinary unit tests. Now let's talk about how Jasmine adds some awesome sauce to our testing efforts.

Jasmine Awesome Sauce
Jasmine is a great testing framework with an intuitive syntax that will help you to optimize your time, organize your code, isolate your units, and communicate results in human readable format. Let's take a quick look at some tests. As we look at these tests you might notice that the tests need optimization. We'll optimize them as we go through the course. In our first glance at Jasmine code you'll see certain functions are used frequently. One of those, the describe function, defines a scope or grouping of tests. It accepts the test parameter describing the grouping, and enclosure containing the steps to execute for the group. We'll see later how the text is consumed by report plugins. The closure will also contain additional subgroups if needed. Since describes can be nested it's simple to create a test structure that's easy to follow. The it function defines an actual test. A text parameter allows us to clearly state the expectation. The closure parameter defines additional steps if any are needed to perform the test, and most importantly, the assertions. The expect function is the assertion, and intuitively defines the actual value to test. Using the fluent interface the expect call is chained using a matcher construct that allows various types of comparisons and modifiers. Custom matches can also be created. SpyOn allows mocking and stubbing of dependencies, making it possible to isolate the unit from other components. With nested describe locks code reuse is maximized for your tests. The beforeEach function is run for each test in the group. The ability to combine the forEach with describe nesting makes tests super simple, some needing only one or two lines of code. Let's run some tests in the console. Test results can be obtained in the console or on a web page. The results can be completely human readable, creating clear documentation of exactly what the code is supposed to do, making it easy to pinpoint problems when a test fails. Now that we've talked a little about Jasmine let's get into TypeScript.

TypeScript Turbo Boost
If you've followed Angular at all you've probably heard of TypeScript. The Angular team uses it to boost their own productivity as they build their world-famous framework. Let's look at why so many people are excited about TypeScript. TypeScript is names for and known for its static typing capabilities. This gives you all kinds of help and documentation in design real time. As I work in my own code that is created in TypeScript I get lots of IDE goodness, but even third party libraries, like Angular and Jasmine have type definitions created by the open source community. That means documentation as you code helping you understand how to use the libraries. That's much better than switching to a browser and searching for documentation. You also get to use ES6 features that all browsers haven't assimilated yet, and future ES7 features. This helps you future proof your code, at least for a little while, and prepares you for what's new in JavaScript. We talked about how costly it is when bugs are found by the user, and how much better it is for bugs to be found during unit testing. Even better is when a bug is found as you type it. That's what TypeScript gives you, immediate feedback on your code, like a trusty pair programmer, but with no bad breath or other awkward side effects. Now let's take a look at our demo project, and see how it'll help us learn to test with these tools.

Virtual Dog Blog Demo App
I have a dog, and he's pretty easy to understand. Let's face it, mankind has been hanging with dogs for thousands of years, so they're familiar, and that's what I needed to help me demonstrate this unit testing strategy, something familiar. My own dog is a malamute, and his name is Zeus. He's really cool, and I want to capture that cool for posterity, so I've created a virtual dog blog complete with photos and interactions you might expect a dog to have, taken from the dog's point of view. I've flavored it with my dog's personality. The dog has several interactions, including those with his master and with objects like sticks and dog food. He also interacts with his environment. He blogs about much of that here. He's interested in the Mars Rover project, so he keeps an eye on the photos that are taken on the planet's surface. He also has interactions with other people, and of course, he interacts with other animals. All of these interactions should help us to demonstrate Jasmine's ability to isolate units and define excellent specifications. Next, we'll see how to set up the project to run in your own development environment, so you can join the fun as we test the virtual dog.

Project Setup
You can use your favorite code editor to follow along with the course. The editor I'm using is VS code, an open source, multi-platform code editor that's particularly adept at optimizing TypeScript; however, many popular IDEs and editors can utilize TypeScript definitions in the edit window, and provide Intellisense, documentation, and compilation warnings. Since this is an intermediate level course I'll assume you've already installed Node and NPM. If not, go ahead and get the latest stable version of Node installed, and with it, NPM. On a side note, if you have an older version of Node, and need to update it, NPM doesn't always get updated concurrently with Node. Check the version of NPM after the update, and if NPM is on a stale version just run npm install npm with the -g command line option. Setting up a project manually can take some time. Fortunately, you won't have to do that. I've created a repository out on GitHub for the project. Search for VirtualDog under the user, TonyCurtisLives. There are two releases available in the repo, one containing the code at the beginning of the course called Begin, and one containing the code at the end of the course called End. You can download the project, open a command window, and run npm install to get most of your libraries. Follow that with npm run bower-typings to bring in the rest of the files needed for the project. To start the server type npm start. Then you can open up your browser, and enter localhost:8042 in the address bar, and that'll get you your virtual dog. Later on we'll discuss the development environment in more detail, and how to run the Jasmine tests. The build related scripts are simplified because I want to focus this course on the actual test code, and keep the remainder as simple as possible. Now that we have our project set up let's dive into our first test with Jasmine.

Your First Tests with Jasmine: Revert Those Asserts
Module Overview
Hi. Welcome back to Testing JavaScript with Jasmine and TypeScript. I'm still Tony Curtis. In this module you'll learn some concepts about successful and not so successful unit testing. We'll introduce Jasmine, write our first tests, and use TypeScript to make it easier to leverage Jasmine to its full potential. First, we'll talk about how to write great unit tests, what works, and what doesn't work. We'll get up and running with Jasmine, and write our first super simple test. Next, we'll test a real component, the MasterController from our demo project, the VirtualDog. We'll describe describes, and use beforeEach to set up our tests. We'll bring out Cousin It to do some hairy testing of our dog. We'll see how Cousin It helps us communicate our expectations, and we'll see what an assertion looks like in Jasmine. Finally, we'll see what happens when tests go wrong. You'll see what multiple failed expectations look like, and you'll see firsthand some of the issues related to poor testing strategy.

How to Make Your Unit Tests Speak the Truth
Mankind has been assembling and using complex systems for hundreds of years, These systems work extraordinarily well, and it's no accident. Careful design, excellent craftsmanship, and precise assembly all contribute to a successful system. You can't just start grabbing some gears and see how they fit together. If you want to have something in the end that's reliable and durable there's a specification for each component of the system. There are checkpoints to ensure each part of the process is successful. First, for each individual gear, then for the interactions between groups of gears, and finally for the entire workings as a whole, but it all starts with the specifications and measurements for each gear. What's the diameter of the gear, the number of teeth, the angle of the teeth, the depth, and width, etc.? These are all specified and measured to ensure the machine will operate and continue to operate with minimum friction, and maximum wear. Software is no different, and these tests in isolation are unit tests. The come early in the process, and are the foundation for the success of the entire system. As I mentioned in the last module, unit tests are a safety net. They give us the freedom to refactor, tackle technical debt, and continuously improve our software. With clear specifications for our code, we'll know immediately when some change has inadvertently violated those specifications. We hit the net when a test fails rather than hitting the ground when a user finds the bug, so let's build a good safety net, so we have the freedom to craft and innovate. Let's talk about some guidelines for writing great unit tests. The following concepts build on each other, and work together to solidify your tests. As a general rule, only include one assertion per test. The test should specify one thing, and report on it. If many things are tested at once you have to unravel the results. Failures and successes can be obscured by a test failure. The test can become difficult to understand, and hard to set up. Keep it simple, one assertion per test. Tests should be designed to provide clear documentation of the conditions and assertions. That's a mouthful. What the heck does it mean? In many testing frameworks that means naming the test with a specific naming convention that states what's being tested, the test conditions, and the expectation. With Jasmine it's easy, and in fact, you can produce a flowing specification of what the test subjects should do, under what conditions, that can be easily read and understood by anyone. You should be able to look at test code, and immediately understand what's going on. There shouldn't be any branching or looping in your tests. The structure should be simple. This is very important when there's a test failure because you want anyone to be able to triage that failure as quickly as possible. When a test fails the test itself is often suspect number one. You want to eliminate the test as a suspect immediately, so make your tests super simple. Give them a rock solid alibi. As you write your tests make them easy to maintain and extend. Apply good coding practices to your tests, and don't repeat yourself. Jasmine makes this easy with a great nesting and grouping strategy. Set up steps can be grouped with tests that need them, making it easy to move repetitious code from the test into a beforeEach or other setup and tear down function. Tests are not just about testing the code. They're about communication. If we change our mindset and start thinking about tests as specifications for our code communication becomes the primary objective. Your test results will be displayed frequently, but your test code is also important for communication. If your test communicates well, and is easy to understand, then it has value in telling the world what the heck your code is doing. If it's convoluted and difficult to read your test isn't doing its job, and runs the risk of becoming irrelevant, a topic we'll discuss later in the course. Jasmine gives you great tools to achieve fantastic communication for your code, and provide an ideal framework for simplifying your tests. Let's demonstrate how easy it is to get started with Jasmine in TypeScript. We want to start off by writing a test for our VirtualDog, so let's take a look at the app.

Writing Your Very First Jasmine Test, Kind-of
One of the first components we interact with in our app is the master controller. The controller contains a list of objects for our master to use while interacting with the dog, and a list of actions the master can do with the object. When we press the button the event is triggered with the action and object, and the dog responds via the blog. Later, we'll look at the code behind this functionality. The demo product has everything you need to get started with Jasmine tests, so you don't need to install anything, but even if you start from scratch with Jasmine it's very easy. You just open a command window in your project folder, and run the npm command to install Jasmine, and make sure to save it to the dev portion of your package. json file. For Angular we also have to bring in angular-mocks. Now let's bring in the TypeScript typings for Jasmine, and again, if you were using Angular the typings for angular-mocks. Now create a test specification file. This is just a normal TypeScript file, and we happen to be storing those in this test/spec directory. Some have noted the value of having the test in with the code that's being tested, and I think that's a good idea, so that's an opportunity for a future refactor, but for now we'll leave them here. So we start off with a reference to the typing files for TypeScript. VS code helps a lot with TypeScript. Here we get a built-in snippet for the reference line. We'll add the index type definition files generated by the typings installations we ran in the command window. This brings in the type definitions for any libraries we're using, like Angular and Jasmine. Now we'll use our first member of the Jasmine library, the describe function. TypeScript autocompletes that for us. Very nice. TypeScript also tells us it takes a description parameter that's a string, and a specDefinitions parameter that's a function that takes no parameters and returns nothing. It also tells us the describe function itself returns nothing, so we know something about the describe function. Let's get some code in there. We'll just put in some quick text for the first parameter. We're just doing a kind of hello world for test, so we don't care at this point what it says, as long as it's not offensive. Then we start on our specDefinitions closure, but wait, since we're using TypeScript we can use the new ES6 fat arrow syntax, which is more succinct, and something many of us are familiar with. I'm definitely using that instead. When the code is transpiled to JavaScript it's set to transpile to ES5 standards, so it'll run in most browsers, but it's awesome that we can use these language enhancements now. Thank you TypeScript. We enter the first line in our spec using the it function, which defines a single test scope. Again, we get code completion from TypeScript, which is a relief because typing it is so hard, but seriously, we get great documentation for this cornerstone function in the Jasmine test framework. It starts off with an expectation string to describe the test scope, followed by an assertion closure that also takes no parameters, and returns nothing. Also notice that the assertion parameter is optional, indicated by the question mark, and a third parameter is also optional, a timeout of type number. This kind of documentation is why we like Typescript's static typing help. We don't need to search the web to figure out what we need to pass here. It's right in front of us. We just enter some quick text here, not worried about it at this point. I've created a snippet in VS code for this. It's used frequently in Jasmine, so it saves some time. The first line of the expectation is, wait for it, an expect call. You'll notice it returns something called a jasmine. Matchers. We'll touch on that in a sec. You'll also notice that the first overlook takes a spy function. We'll talk about mocking in the next module, so let's take a look at the other overload. Looks like it takes something called an actual of type any, meaning any type can go there. We use that, and I'm just going to put true there for grins. Don't really care for this quick demo. Since it returns a jasmine. Matcher we're going to look for something that works. The matcher starts with toBe, so let's look at this. I wish they'd make a to be or not to be matcher, though it may be a bit confusing, and potentially fatal. Since there's a toBeTruthy matcher we'll use that. Notice that the matcher takes an optional parameter called expectationFailOutput of type any. Normally that parameter isn't used, but you can use it to clarify what the heck's going on with your test when it fails. The failure message reported by Jasmine isn't always clear, so you can add a string here to communicate something about the failure. So now we have our hello world example. Let's save it. Remember that this code is TypeScript, so it needs to be transpiled to JavaScript. If you run npm start you'll invoke the TypeScript watch and compile on file change. You can also go to your project folder in the console and enter tsc -we. Here we see the watcher watching and compiling. Next, we'll set up a simple spec runner page to run the tests, and display the results in a browser.

Your Jasmine Test Results in the Browser
Now let's look at the SpecRunner file we use to run the tests and display the results in the browser. It's very simple and easy to set up. You have some reference to Jasmine style elements and libraries, as well as your bower and node libraries, and your app libraries. At the bottom we add the reference to the spec. Open the file in the browser, and you should see some results. Wait, what? No specs found. Let's take a look. So what happened is I actually forgot to add the spec suffix to my spec file. Hey, why didn't you guys tell me? I know you saw it. Anyway. If you end up wondering what the heck happened to your tests it's often related to these references in that SpecRunner file. So I fix that, then reload the browser. You see right away how many specs were run, and how many failed. You also see a cascading list of the describe and it's text. Notice this is what we entered in the string parameters for the describe and it functions. You also see how much time it took to run the test. At the top you see what version of Jasmine is being used. This dot here actually expands to one dot per test. Since we only have one test there is one dot. These are color coded for passing, failing or ignored tests. The describe and it text contain links to run the tests, so clicking on them will run all tests nested in that describe or a single test if you click on the test description. As we grow our test suite you'll see the value of those links. The Options button allows you to select options for your test run. We'll discuss those in more detail later in the module. So that's our first test in Jasmine. Pretty easy, but we're not testing anything. Let's dig into that masterController and really test something.

Testing the MasterContoller: Using Describe to Describe
First, we get rid of this little example here. Then we'll open the code for masterController. The Angular code is also written in TypeScript, so you'll see lots of type annotations, and you'll see I'm using ES6 features like class, implements, and fat arrows. You'll notice this file contains two classes, MasterAction and MasterController. Let's focus on MasterController first. I'm not going to do a deep dive into the ES6 features TypeScript makes available. I'll just briefly touch on them. The ability to create classes is a language enhancement from ES6. As adoption of ES6 increases you'll begin to see classes used in Vanilla JavaScript. JavaScript classes may contain public and private members, like what we're used to in Java, C#, and similar languages. The JavaScript classes also have constructors, and you can see how they're implemented here. TypeScript adds a shortcut for declaring and initializing class members right in the constructor. By adding the private or public keyword in the constructor's parameter list you're actually converting the construction parameters into members. This constructor takes two parameters, rootScope and eventNames, which are both converted to private members of the class. For testing we'll want to inject at least one of these parameters, and Angular's dependency injection will help us do that, as you'll see when we write our tests. The constructor also sets a public property and calls initializeLists. We'll look at initializeLists below, but first let's finish looking at any other public members we're interested in testing. We'll only test public members, just like we do in Java and C#. Looks like we also have two public methods, throwSomething and feedTheDog. We'll add these to our tests as well. Notice the only thing they do is broadcast an event. We want to talk about these in depth in the next module when we discuss mocking, so we'll add pending tests for these. That'll give us a chance to talk about the value of Jasmine's pending test feature. Down in initializeLists we initialize the masterActions and masterObjects arrays by pushing various items. The masterActions are Throw Object and Feed. The masterObjects are a short list of things a dog might chase or eat. Pretty simple controller. So in our test let's start with a describe again, but this time we'll talk a little about these describes, and what they're doing. The first parameter is the description, and we'll follow convention for the text we use here. Descriptions should make it clear what we're testing. We'll start by describing the file we're testing. Seeing this description on its own may seem a bit odd, but when we add the other descriptions it'll make sense. The second parameter's a function that will contain variables, set up and tear down functions, and the tests themselves. It takes no parameters, and returns nothing. The function can also contain nested describe statements to divide the specification into smaller, more focused segments, so first we're going to set up a skeleton of describes to show how they nest to form a nice, flowing specification. We'll start with the masterController. I often use the term specification rather than just calling them tests. A specification implies a description of what's expected, something measurable and cohesive. Next, we'll add the masterController's constructor. As we nest these describes you can see how our specification is expressed in plain English. With these description parameters we want to clearly communicate two things about our test. First, what we are testing, for example, the file we're testing, the class or the method, second, the conditions we are testing. For example, if we're passing in certain parameters or we've changed the environment for the test, we would convey that condition in the description. Let's add throwSomething. Now you'll see good descriptions of what we are testing in our first tests here. You'll see good description of conditions in a later example. Now we'll add feedTheDog. We also want to test the MasterAction class, and we're only testing the constructor here, so we include that in this description. Now we have a skeleton of describes, and it's very readable. We are clearly testing two different test subjects, and it's clear what we're testing on those two classes. Next, we'll put some meat on the bones of the skeleton by setting up our environment to run the tests.

Setting up Our Tests for Success
Now let's start filling in some of the variables we'll need for our spec. We'll use the let syntax from ES6, and set up a variable for our sut, which is an abbreviation for system under test or subject under test. This is a convention I like to use as I write unit tests, so when you see sut think system under test. We'll set the type of the sut by getting the controller type from our namespace, dogsrus. virtdog. TypeScript typing uses a notation that's different from what we see in Java, C#, C++, and similar languages. Instead of adding the type as a prefix it's a suffix preceded by a colon. The type can be one of the existing JavaScript types, like string, Boolean, number, etc. or it can be a type from your own or a third party library. Notice how TypeScript helps us with our own code now. This is why we use static typing, and it'll continue to help us throughout the testing process, so we'll leverage that typing as much as possible by adding static typing wherever we can. Since we're going to be using the dogsrus. virtdog namespace all over let's import it, and give it an alias that's more concise. Now we're going to set up variables for anything else we need for testing. We'll need one of the constructor parameters, rootScope. See how TypeScript helps with Angular 2? Which is awesome. Makes this a whole lot easier. We'll need the controller service from Angular to allow us to get an instance of the controller. Angular's dependency injection is built for unit testing awesomeness, allowing us to inject our own objects into the constructor. We'll see how that works later. We'll also want a parameters object to pass in the parameters to the controller service above. We'll call it masterControllerParams. We add slots for our two construction parameters. All of these variables are now available to our test code inside this Master Controller describe scope. They're not available to the MasterAction describe scope, and this is exactly what we want, and let's use a feature of Jasmine we haven't used yet, the beforeEach set up function. Set up functions allow you to prepare variables and objects for the test. They're run before each test in the describe scope. You can see it takes a function that takes no parameters and returns nothing. No description string here. It also takes an optional timeout parameter that's numeric, but we won't be using that. Since we're running in Angular we need to bring in some mocking for our Angular module. This mocking is all handled behind the scenes by the Angular mocking framework. We also need to be able to set some references to the Angular services. Angular's inject will help with that. These are all Angular specific features. Don't confuse them with Jasmine. The ng namespace indicates that these are Angular bits. We're going to pass in an injector service that we'll use to grab references to our Angular services. First, we'll get the controller service, so we can instantiate the controller. We'll use the injector's get function, and notice how TypeScript has identified a type parameter for that git call. Type parameters are what we use for generics in TypeScript. When we have some member that can return anything we can type that return by adding the type parameter. Here we just need to pass in the ControllerService type. Notice how we're also getting an example of how to use the name parameter. Nice work TypeScript. We'll do the same for the RootScopeService. Next, we'll set up masterControllerParams with rootScope and eventNames. For eventNames we'll use the configuration variable from our namespace. Since it's just a list of static values there's no need to mock it. Now we'll instantiate the sut, our controller. We use the controller service to do that. If we look a little closer it looks like the controller service needs a type parameter here. This makes sense, since the controller can be any type. Thanks to the definitely typed community, the type definitions for Angular often include type parameters, so we can also maximize TypeScript here. We provide the name of the controller, and pass in the parameters for construction. So now our set up is complete. All the variables in this beforeEach will be refreshed before each test for the MasterController is run, wiping the slate clean for every test. Nice. Next, we'll write some tests with the it function, and set some expectations with expect.

Time to Bring out Cousin It
If we're going to test in Jasmine we have to bring out Cousin It. In the 60's there was a sitcom called The Addam's Family featuring very odd and macabre family members. One of them, Cousin It, was pretty much a mass of long, straight hair with no other features sporting some sunglasses and a hat. Jasmine's it is a lot less hairy, not at all mysterious, and rather than being a supporting cast member, is basically the star of the show. The first parameter, the expectation, takes a string describing the expected result of the test. Coupled with the describe strings mentioned earlier, you now have a complete description of the test. The describe strings tell us what we're testing, and under what conditions, and the it string tells us the expected results. This kind of intuitive test description makes it much easier to understand the test results. This helps us understand what the test subject is specified to do, and helps us troubleshoot when a problem occurs in the test. We'll start by testing the familiar name, and we'll use the convention of starting our it string with should to make this specification more readable. You can see how easy it is to understand what this test expects the constructor to do. The second parameter is the assertion, and as you can see, it's optional. An it function without an assertion is a soon to be appending function. We'll talk about those later. Within the assertion closure you can perform additional set up steps, execute the step that triggers the test, like running a certain method, and check the results with an assertion. Depending on how much set up you have in the beforeEach functions, you may only have an assertion in the it closure. That's the case for us since we're testing the constructor and instantiating the sut in the beforeEach function. For our assertion we use the expect function. Jasmine decided to name it expect rather than assert, and it does seem to make the test language very intuitive. We won't use the overload that takes a spy. We use the one that's looking for an actual value of any type. Let's put the actual value from our sut in that parameter. Next, we have a matcher. Let's look at our options there. ToBe would actually work here, but I prefer to use a different matcher. Let's look at the next one. ToBeClose isn't quite what we want, though it does sound like a carpenter song. ToBeDefined tells us if a variable is defined. ToBeFalsy checks for the JavaScript definition of falsy. Most of these other matchers are self-explanatory. You'll notice some mocking related matchers to detect when a mock has been called. There's also matchers to check for thrown errors. The toMatch matcher allows you to do a regular expression comparison against a string. The matcher we want is toEqual. For literals it doesn't matter whether you use toBe or toEqual, but if we're testing objects remember toBe is testing the memory reference to determine if the object is the same by reference, whereas toEqual is check for equivalent value. Generally, you use toEqual, but we'll see examples of both. The expected parameter is the value that our actual will be tested against. This creates a very intuitive pattern. Expect my actual value to equal this expected value, so our first real test is done. Let's write another. We'll make sure speciesName is being set to Homo Sapiens. That was really easy. Now I'll test some lists. We know masterActions has two items, throw and feed, let's check for those. First we make sure we have two items. Then we're going to check the names of the items. It seems like we could use the toContain matcher here, but it checks for a specific object instance. What we need to know is the name of the object, so it won't quite work. What we're doing here is an optimal, and we could create a custom matcher to do this, but for this simple test we don't want to go to that trouble, so we'll check for Throw Object and Feed in the masterActions list. Next, we'll make sure the action function is being set. To do this we'll actually need to use two asserts in one test. Sometimes you have to do that. In this case, to know the function's been set we need to know that it's defined, and it's not null. There will be exceptions to the rule of one assert per test when it makes sense. There's a toBeNull matcher, but we need to check for NotNull. I see a toBeUndefined matcher, but not a toBeNotNull matcher. Luckily there's the not modifier, another great piece of Jasmine's fluent interface. You can see it returns a matcher allowing us to chain it, so we can apply not to any matcher. We finish up with the toBeNull matcher, and we'll throw in some descriptive text, so if it fails we know a little more about what went wrong. We'll do the same for the feet action (Typing). Let's finish up by testing the masterObjects list. Our master should have the objects he needs to interact with the dog. There are six things, and I remember all of them because I have such a stunning memory, but you can go take a look in the master controller if you want to see the list yourself. We're not going to test all the details, just the object names and how many there are. So first, we test how many, then we test for the object name. I'm just going to make a little template here, so I can repeat this test for the six objects. Now I'll just fill in the rest of the names, and the indexes, and there's our constructor tests. Let's collapse our code, and take a look at how our specification is flowing. As we read the specification it's very easy to understand what's going on. The nested descriptions, it expectation strings, and the expectation functions really spell it out for us. Now let's go ahead and finish up the rest of these tests. For the throwSomething method we know that it should broadcast the throw event name, but remember, we're not going to complete this test until we get into the mocking module, so we'll go ahead and create the test, but for the body of the test we'll insert a pending function. This allows us to track the fact that we have a test to write, and gives us a nice little reminder. We'll do the same for the feedTheDog method. Now let's move on to the MasterAction. Again, we'll need an sut or system under test, but this time it's the master action that we'll be testing. We're going to need an action function to instantiate our master action, so let's create one of those. We'll use the ES6 fat arrow syntax here to create our function. We'll statically type the parameter to match our MasterAction's actionFunc, and we'll use an empty function body because what we really want here is a stub. We don't want it to do anything. We'll talk about this more in the next module, and we'll show you how to throw some brains into this stub to transform it into a mock. In our beforeEach function we'll instantiate the sut, passing in a name, and the action function we created above. Our master action should set the action name to what we passed in the constructor. It should also set the action function to the instance that we passed in the constructor. For this test we'll use the toBe matcher to make sure the function in the master action is the exact same instance as what we passed into the constructor. This is precisely why you use the toBe matcher, to check for the exact same instance of any object. One more thing I'm going to do. Get rid of the object up here in the describe. I don't know why I put it there, and it doesn't match what we have for the master controller, so let's remove it. Okay, let's run this test. Eighteen tests, 2 pending, 0 failures, 16 passing. Look at how the spec reads. It's crystal clear what we're expecting the constructor to do, and if any of these tests were failing we would know right where to look. We also know that the throwSomething method has a pending test, and why it's pending. Same for the feedTheDog method, and we also know what the masterActions constructor should be doing. This specification is doing a great job of communicating what our test subjects are about. Now let's see what happens when tests go wrong.

Tests Gone Wrong
So we have this easy to undersstand test that takes advantage of Jasmine's excellent testing framework. Now let's look at an example of what happens when tests go wrong. Here we have a test for the master controller, and in some ways it looks similar to what we already have, but if you take a closer look you'll see there's only one describe in this test. There's also one before each, and there's only two tests. In some ways it looks a little simpler than what we originally had for testing the master controller. Let's run these tests, and see what they look like. Okay, we have a couple of passing tests. Not sure what they're telling us, something about properties for the masterController and MasterAction properties, and they're green. Okay. Everything is awesome, but what would happen if there were errors reported? What would that look like? In fact, I would say we have a plethora of expectations here. What happens if they all fail? Let's make them fail, and I'm not focusing on the reason for failure here. It isn't important at this point why they fail, I just need to demonstrate what it looks like when they do, so I'm going to quickly introduce some failures. Now let's run the tests again. We're told we have two failures. Looking at the details we show a failure for the first expectation in each of the two tests, Dillon and the testAction name for our MasterAction. In our tests you see that Dillon is the first expectation for masterController, and the testAction name is the first expectation for the masterAction object. For unit testing frameworks this behavior is normal. Once the first assertion fails the test is aborted. This obscures the outcome of the other assertions. In the master controller there are 16 things we don't know about our expectations. We only know the first one failed, but Jasmine has a way around this. You can go into options, and uncheck stop spec on expectation failure. If we rerun our tests we can see that we now have a _____ of error reports. If we go back over to our spec list it still shows two failed tests. There's no summary of what expectations failed. There's not even a count of the failed expectations, so unless you want to scroll and count you don't know how many expectations failed. This makes it hard to figure out at a glance what failed here. You have to go swimming through this river of failures, and try to track down each cause, and the expect text can be a little cryptic, like expected 2 to equal 3. If I told my high school math teacher that he'd pack his bags, and look for a different job. So we're not getting much help here. We have a ton of errors, and quite a bit of work to wade through them all. Our summary is telling us properties and MasterAction properties are being naughty. Not a lot of info here. Remember when I said tests are about communication? There's not a lot of communication on this summary page, and there's almost too much information on the details page. Looking at our test code there are some other problems. We're testing two different classes here, but the full list of variables is available to both tests. That isn't necessary, and can cause confusion for larger test suites. Also, since beforeEach runs prior to each test we're bringing in a bunch of set up for an angular controller that isn't needed for the MasterAction tests. It's like we're throwing everything into a big cauldron and stirring with a stick. Let's get this straightened out before something nasty pops out of the bubbling cauldron. So now we're back to our original test, but to be fair, we should introduce the same errors here as we had on the other test to see the difference. I've already added the errors, so let's run the test. Take a look at the summary. Wow. At a glance I know 16 times as much from this defect summary as I knew from the other defect summary. In fact, more than that because the other summary was literally eating my brain like a zombie. Even in the detail you're getting meaningful documentation for each error in the header of the detail. Now let's go back to our test and fix some of the issues (Typing), so we have a mixture of passing and failing tests. Now let's run the tests again. Way easier to see what's going on. We know exactly what is failing. That makes it much easier to dive in and start triaging our failures.

Summary
Let's do a quick recap of what we learned in this module. We talked about some of the specifics for writing great tests. We saw the bare bones minimum needed to create and execute a Jasmine test. We spent some time describing the describes, and why those description strings are so important. We set up our tests using the beforeEach function to maximize our code reuse. We finished up our test using the it function to explain our expectations, and by using the expect function to report the results. Then we saw what happens when tests go wrong. Not a pretty sight. Next, we'll take a look at how to isolate our system under test by mocking our dependencies.

Mocking with Jasmine
Overview
Welcome back to Testing JavaScript With Jasmine and TypeScript. I'm Tony Curtis, and you're not. In this module we'll talk about the importance of isolation in unit testing, and we'll explore how the Jasmine framework enables dynamic mocking and stubbing. We'll also see how TypeScript generics help us with our mocks. First, we'll cover the concept of dependencies, and how to alter those dependencies for isolation. We'll discuss some concrete examples of mocks and stubs. We'll learn how to create dynamic mocks and stubs using Jasmine, and where to use them in your tests. We'll learn how to spot the difference between a mock and a stub when you look at a Jasmine unit test. We'll cover some different strategies for both creating and exploiting mocks, including setting return values and techniques for handling tricky interactions between objects, and we'll learn what can happen when we fail to isolate our unit tests.

Of Mice and Mocking
So why do we mock? We use mocking to generate both mocks and stubs to help isolate our test subjects. Jasmine has its own dynamic mocking framework, making it easy to create mocks and stubs without coding our own. How do we use these mocking frameworks? Well, we favor stubs whenever possible to focus our tests on the end result, not on incidental interaction. When interaction is part of the specified behavior we test that with a mock. You can easily get carried away with mocks, so we avoid testing the details of how we got the result. As I was thinking about testing and isolation, and how things can be interconnected I thought about a game I played as a child called Mousetrap. It's a simple board game with a Rube Goldberg twist. In the game you have a mouse, and try to catch the other players' mice without getting caught yourself. When you start the mouse trap isn't built yet, so you add pieces as you move along the board. Finally, the trap is built, but this is no ordinary mouse trap. It's designed to lull even the craftiest of mice into a sense of security through abstraction. I mean how could that little crank over there have anything to do with the end of my cheese eating freedom. I love this game and hold it partially responsible for me becoming an engineer. For those who don't know, I'll quickly explain how it works. The crank is turned, which turns a gear, which turns a gear, which pulls back a lever with a paddle on it until the paddle is released with great force, striking a boot, which then kicks a bucket. I love the subtle humor there. The bucket contains a metal ball that spills out onto the stairs, rolling back and forth on each stair all the way to the bottom. At the bottom the ball rolls into a rain gutter and follows a curvy downward path until it strikes a strange looking stick, which starts the ball at the top to roll down a plank and through a hole. It lands in a bathtub and rolls down a drain hole, landing on the upside of a teeter-totter. On the downside of the teeter-totter is a man prepared to dive into a round tub that contains no water. He should have thought that one through. He dives into the empty tub, his head hitting the bottom, dislodging the bell shaped cage, which rattles down the spikey pole. The mice can only watch in awe as it descends on them, catching them as deftly as any plastic mouse has ever been caught. Lot's of moving pieces here, lots of interaction, so let's say we want to test the system. We can do an end to end test when everything is assembled, but what if we just want to test the stairs? If we isolate the stairs we can focus on what we need to test. The stairs receive an input, a metal ball or specific size and weight, supplied at a certain point at the top of the stairs from another component, the bucket. The stairs also supply an output, a change in state of the ball in elevation, location, velocity, and direction. The stairs also pass the ball to another component, the curvy gutter thing. We want to test the stairs, but if we use the bucket we have to tip the bucket, so we need the boot, and if we have the boot we need the paddle, and the gears, and the crank, and at the other end we need the curvy thing to catch the ball or it goes rolling away, but we don't want all that. We want a simple test of the stairs. What if we just place the ball on the top of the stairs? Well it's as likely to fall off the back of the stairs as it is to go down the steps. That's where mocks and stubs can help us. They have the same interface as what they're replacing, but not all of the functionality, so let's start with a bucket stub. It's fixed in a downward position, so it can't swing upright. It also has a hole in the bottom where we can place the ball, so it'll roll out onto the stairs. So this isn't your original bucket, but the interface that delivers the ball to a certain point on the stairs is the exact same. The stub does not directly affect whether the test has passed or not. It's just a tool to help us, a placeholder for something the stairs require to function properly. The curvy thing connects to the bottom of the stair. It receives the ball from the stair. Here we need a simple mock. The mock is necessary because it receives and stores the ball, proof that the ball was passed. The interaction of the stair delivering the ball is a thing we have to test, so we need a mock. Our mock is not going to send the ball downhill. It's just going to catch and hold the ball, so it's very from the gutter. It's much shorter and has no outlet, nowhere for the ball to roll to, but the interface with the stairs is the exact same. So in our overly simple example the bucket is like a stub that provides a value to our test subject. The ball is data that'll have some state changed by the test subject. The gutter is a mock that'll be called by our test subject, passing the data it received from the stub. Now we can test our stairs by providing the data to the stub, and afterward checking the mock to see if the data received was the same as the data provided by the stub. Let's apply these concepts to some code.

Jasmine's Super Secret Agent Man
We'll go back to the master controller. Remember in the last module we had these two functions, throwObject and Feed that we didn't test. Now it's time to test them. The only thing these functions are doing is broadcasting the event, so we can't test for any state change or return value, we have to test the interaction with the broadcast function. To perform this type of test, and keep the test subject isolated we're going to use an exciting feature of the Jasmine framework, the spyOn function. SpyOn lets you act like James Bond and intercept sensitive data before the object being spied on ever gets it. You can then do a few things like examine the data, stop the data from going to its intended target, better known as isolation, and set a return value for the call. In this module we'll explore both stubs and mocks. When a stub is sufficient we won't examine the data passed with the call if there is any. We'll just provide the stub to fill the gap in the required components for our test subject. We can also have the stub return a certain value if the test subject is expecting a return value. For cases where we're testing interaction behavior we'll create a mock that knows it was called, what parameters were passed, even how many times it was called, and we'll verify the important interactions. Let's start fleshing out these two tests. Let's start fleshing out these two tests. First, we need another describe. Since we're testing two members with the same setup, why do that twice? Just create a new describe scope, and a new beforeEach. The describe string will add further explanation of what we're testing. Both of these specifications are for a property that broadcasts some event. We need to create an object to send with our broadcast. We don't care about any of the properties for this object. It will simply be passed to the broadcast call. Since we'll be mocking the broadcast call none of its code will be executed, and none of its properties evaluated. This is one reason we like isolation. It removes interaction side effects. Now we create a beforeEach. We'll set our broadcastObject, and we'll set up our spy. There are many ways to create a spy. We'll use a simple technique for this example. Using the spyOn function we pass in the object to be spied on, and the name of the method we want to spy on as a string. This creates a short circuit for the rootScope object, ensuring that when we call broadcast the call will be intercepted, and won't be passed through to the real broadcast function. Let's move the describe scopes for throwSomething and feedTheDog to nest in the one we just created. Now I'm going to rewrite this describe string to remove the method word in the description. I initially had these two as methods of the class, but changed them to properties of type function. Now let's add something to our expectation description. You'll see why in a minute. You could eliminate the describe and add the what you are testing in the expectations string, like throwSomething should broadcast. We'll show an example of that technique later, but for this example we'll keep the what we are testing out of the expectation string. Now we'll delete the pending function. We need to call the throwSomething method here, and pass it the broadcast object we created above. Next, we set up our expectation. Remember our first expect overload took a spy with the type of Function as the parameter. That is what we'll pass here. In our setup we created a spy on rootScope. broadcast. Enter that here. We are testing the interactions, so we need to know what happened with our mock. Now we want to use the matcher toHaveBeenCalledWith because we want to know that we broadcast the right event and the right object. It is looking for both parameters, so we add the eventNames and the broadcastObject. Now we do the same for the feedTheDog property. Let's collapse the code and read this back. In the file, masterController. ts, the MasterController's broadcast related property, throwSomething, should broadcast the throw event name, and the object thrown. That's pretty easy to understand. Let's run the MasterController test, and take a look at the results. Our tests are passing, and the spec results are easy to read. Let's take a look at what would happen if there were problems with these two methods.

Testing Our Tests, Trusting Our Spies
I've introduced a couple of errors in the MasterController. In the throwSomething property I'm broadcasting the wrong event, and in the feedTheDog property I'm sending an empty object rather than food. Not a good thing for my dog. Let's rerun the tests now, and see what they look like. Now we have some errors. If we look at the summary it's easy to see where the problems are and how many we have. In the details we need to look in this very long line for the description. It's clear what's expected, but what is the actual? One quick way to find the actual is to just search for it. Here you can see the actual value for the first parameter was catHiss. For the second failure the actual value for the first parameter was good, but the second parameter was an empty object. No food for Zeus. So our tests are doing a great job of communicating in the summary and in the failure details. Let's go back to our tests. It's clear our mock is inspecting the call, but how do we know it's truly isolating the call? Well let's test it quickly. These tests are temporary, so I want to make note of that. We'll add a new describe. This describe text will state what we're testing, as well as the condition we're testing. We'll add some variables to use in our test. We need a foodObject and a Boolean to test whether we broadcast the event. Before each test we set the food object to a new DogObject, and set wasBroadcast to false. Now we'll spy on broadcast. Then we set up a listener for the masterFeed event to set the value of wasBroadcast to true. Now I'll describe for the no callThrough condition. This is another example of where we use the describe to state the condition we're testing. Let's make our test for not broadcasting. First we feed the dog, then we expect broadcast toHaveBeenCalled. We're not testing for what it was called with here, we just want to know that our spy detected the call. Next, we make sure wasBroadcast is false. Done. Now to prove broadcast is working at all, we'll test if we get a broadcast when we set the spy up with a callThrough. We can add a callThrough to the spy after the spy is created. This will show us how to use TypeScript typecasting. Since a static environment can't tell that our rootScope broadcast has been decorated with a spy we have to cast it to a Jasmine spy. Now we feedTheDog, check that broadcast was called, and see if the broadcast was passed through. Let's run the tests. So we have two failures, but those are the errors I introduced in the code that I haven't fixed yet. Jasmine will hound me until I fix those, so no worries, but you can see our spy tests worked perfectly. So now you know that our spy will keep his secrets unless you explicitly tell him to do a callThrough, and that's how easy isolation is in Jasmine. Now we'll look at some other options with spies, like creating a spy object, setting return values, and verifying how many times a spy is called.

Examining the DogController's Interactions
We're going to be testing some interactions with our DogController. Specifically, we'll test what happens when the masterThrow event is triggered, so we'll look at this diagram of those interactions. The DogController receives static values from two configuration objects passed into the constructor. Those are one-way, read only interactions. Another interaction is with the masterThrow event. The DogController has a private method called fetch, which is a listener for the masterThrow event. The fetch method receives the event as a parameter when the event is triggered. Again, this is a one-way, receive only interaction. We won't mock any of these dependencies. The last interaction for these tests is with the DogObject that also gets passed as a parameter when masterThrow is triggered, and we will mock that. The fetch method receives the DogObject, then calls the DogObject's chewOn method, so we'll spy on chewOn. We'll test the call to chewOn, and since it returns a value called chewExperience, we'll have to provide a return value from our _____. We also have to handle a special situation where _____ CHON was called multiple times, and the return value may change within that group of repeated calls. Let's take a look at our DogController. The DogController is the main object in our application. It listens for several events and responds by changing the state of the dog and blogging about it. It has several properties. Many of these aren't implemented yet because it's a training application meant to be expanded and extended. Its constructor takes an added service, so it can interact with events, and takes a dogConfig object. We already have tests for the constructor, so we're not going to spend time on those. Let's take a look at initializeEvents. Event handlers are something you want to test because they do represent a public interface into your code exposing the functions that are wired up as handlers to external calls. Near the top of this list are two events we're very familiar with, masterThrow and masterFeed. For the DogController we already have tests for masterFeed. It looks like the masterThrow event listener calls the fetch function. Let's take a look at that. Looks like we're passing an event, and we're doing nothing with that event, so no need to mock that. We're also passing a DogObject, and most of the interaction with the DogObject comes from the properties name, flies, and chewy, but if you look closer you'll see we're also calling chewOn, and not only that, chewOn returns a value. Let's take a look at our dogObject. Looks like our dogObject has some associated enums. The dogObject also has several associated properties to save state. Let's look at the constructor. Looks like the constructor has some logic. When there's logic in the constructor, other than just some simple assignments, it's generally a good idea to mock the object. You don't want you test to run through unnecessary logic. You want it to be as simple as possible. Let's look at the chewOn function. Looks like it's changing state when the object gets chewed, which makes sense. Things will get more slimy and chewed up as the dog continues to chew. This is a good example of why we want to isolate our DogController from the dogObject. We don't want to run through all this logic to test our dog. Looks like chewOn returns a chewExperience, but that chewExperience could actually change in the logic. Next, we'll look at the existing specification for the DogController.

The Spec Setup and TypeScript's Specialized Overloads
We already have a spec for the DogController. Let's open it up. There are a number of tests written here, but we'll be adding tests for the masterThrow event handler. Here's the setup for our spec. It looks similar to the MasterController spec with a few additions for the dog controller parameters, like InternalService and dogConfig. Before we write the tests I want to take a moment here and clarify something I talked about in the last module. To do this we're going to have to do a brief deep dive into TypeScript typing, but don't panic. I just want to explain what's going on, so you understand what you're seeing when you run into it in the future. The injector. get function has an overload with the type parameter, as shown here, and I've used these in the previous module, but there's also a shortcut. The Angular definitions contain overloads that actually define a string literal for the type of the desired service. For example, $controller, and others as you see here, and notice how the return type buries according to the name string. Now this isn't want you'd expect to see in a type definition. It's not showing a type at all. It's a string literal. Wait, what? let's take a look at a type definition file. In VS code I hit F12 to navigate to the definition. Here you can see we have these type definitions with string literals rather than types, this is called a specialized overload, and must be used in conjunction with a normally typed or non-specialized signature. If we go to the top of this long list of overloads we'll find the normal signature, and if we comment this out, you see here. So don't freak out when you see some string literal as a type, someone who's just trying to make your life easier. In our case, we don't have to use type parameters, just insert the string, and we're golden. Further down we have the startDog that's passed in to initialize the dog property. As I said before, we already have tests for the constructor and the masterFeed event listener. In the SpecRunner we already have the references we need to run the dogControllerSpec, so we're all set up to create the throw event tests. We'll do that next.

Using Stubby Spies for Filling Gaps
So let's write some tests. So we'll start a new describe here for our masterThrow event listener. We need a variable for our throwObject, so we can pass it when we broadcast. In our beforeEach we'll instantiate that throwObject, but we won't use the DogObject constructor. Instead, we'll use the jasmine. createSpyObj function. The first parameter is the baseName, and it's a string. We put the name throwObject here since that's the name of the variable we want to create a spy object for. The second parameter is an array of method names. Each method in the array will be set up with a spy. This is a great way to create a spy object when you have several methods you need to spy on, but it also works great for what we want to do. For our case, we just put in the one method, chewOn. Now we'll fill out the rest of the properties. I like to start out with default property values, then I change them as I need to with varying test conditions. Finally, we set a return value for our chewOn spy. We'll have to cast it to a jasmine. Spy since our chewOn member has no idea it's a spy. If you remember, we had to do this in some of our previous tests. We'll set the returnValue to a fair ChewExperience. We'll also set the blogContent to empty, since we'll often be testing that, and we want it to be empty prior to the throw event. That's all for beforeEach. Now let's write a test. Our masterThrow event will trigger our dog to blog the word master. To trigger the test execution we broadcast the masterThrow event, passing in our throwObject, and we expect our blog content to contain the word master. The throw event will also cause our dog to blog about the thrown object, so we'll look for object name in the blog. In the next test we're stating some conditions in the expectation string. This does make for less clutter in your test code because you don't need a scribe for every condition. Our object flies property is set to false, so in that case our dog will blog about snapping it up. For this test we'll flip this condition, and test what happens when our object does fly. Finally, we'll test what happens when the object is chewy, but not in the dog's chewObjects collection. Our dog should add the object to his list of objects to chew on. In these tests all our spy is doing is supplying the return value from chewOn, and the other property values we've set. So our spy is really behaving like a stub rather than a mock in these cases. Let's take a look at how this is working in our test results window. Our tests are passing, and it's easy to read what should be happening. Next, we'll show you how to change the return value of the spy, how to return a sequence of return values, and how to use this stub as a mop.

Wise Spies for Testing Interactions
Now back to our specifications. We're going to start this next block of tests with a describe, and we're going to change what our spy returns when chewOn is called. We use a beforeEach to change the returnValue, and fortunately, changing the returnValue is exactly like setting the returnValue, so it's very straightforward. It's nice to know that you can change that return value on your spy any time you need to. For our test the dog will blog squeak if the objects chew experience returns a squeak. For the next test we'll use our spy as a mock, and actually do a different kind of test. We'll use the mock to record how many times it was called. The inspiration for this OCD chewy behavior comes from my dog Zeus. He likes to chew on things, but if it's squeaky he goes a little crazy. We'll set the value of the squeakyOcdChewCount, then we'll broadcast our event. In this expectation we won't test the system under test, we'll test the spy. Any time your expectation doesn't test your sut it should be testing a mock. This is also how you know that your spy is acting as a mock and not a stub. We're going to test how many times our mock is called. It should be called once for the initial chew, then an additional number of times depending on what the squeakyOcdChewCount is. Awesome. Now I can test my dog's chewy OCD behavior. What could be better? For our last test we're going to test what happens when the chew experience changes from squeaky to something less than squeaky. Again, I've mimicked my dogs own behavior here as he'll continue to chew hoping to get some squeakiness out of the object. For this test we'll set up our spy with returnValues. Notice the s, so we can simulate what happens as the dog object changes its state. The object, when it has reached its chew limit, starts to degrade, so a squeaky dog toy will stop squeaking at some point. We simulate that by first sending squeaky, then sending one step down from squeaky, which is still a great chew experience, just not squeaky. We set the OCD count and broadcast the event like we did above. Then we test blogContent to make sure it contains try again. Let's run the tests. We have a couple of failures. Let's look at the Spec List. Well our squeak blog didn't work. Neither did the try again blog. Let's go back to the failure detail. Yeah, it didn't blog anything about the squeak. I think I have a problem in our code. That's awesome though. This is what unit testing is about. Let's look at our fetch code in the dogController. So it looks like when chewOn returns a squeaky experience we're calling chewOnSomethingSqueaky, passing blogEntry and fetchObject, so let's look at chewOnSomethingSqueaky. So it adds to the blogEntry, and there's the problem. BlogEntry is passed by value not by reference. We need to refactor chewOnSomethingSqueaky to return a string. So we return blogEntry, then down in fetch we add the return value to the blogEntry there. Save and compile, and rerun the tests, and all the tests pass. Great. We have easy to understand specifications, we are testing interactions, and we have isolated our test subject, so that we don't have to run any unnecessary logic to complete our tests. Next, we'll take a look at what happens when you don't isolate your code.

We Don't Need No Stinking Mocks
Okay, so I'm just going to show a couple of new specs I added to our specification. We still have the throwObject variable to pass into our broadcast event. In our beforeEach we're instantiating that throwObject by getting a new DogObject. We didn't mock the object, we're using the real deal. We set the chew experience to squeaky, and again, we cleared out the blog content. Our first test just expects to blog squeaky. Pretty simple. For our next test we've changed the environment a little, using a ChewExperience of fair, and setting some other values. Then we're broadcasting, and of course, since our true experience is fair we're not expecting a squeak. Now run the tests, and see what they look like. Both tests are failing. That's weird. They were ultra-simple. Let's look at the detail. In the first test we expected to see squeak, and it isn't there. In the second test we didn't expect to see squeak, and there it is. What's strange is the other dogController throwEvent tests are working fine, so why did these two fail? Let's take a look at the dog controller, and examine what happens to cause it to blog squeak. Looking at the code it doesn't blog squeak in the fetch function, so it only blogs it in the chewOnSomethingSqueakyFunction. The only path to get to chewOnSomethingSqueaky is if the fetchObject's chewOn returns squeaky. So let's go look at fetchObject. In the first test we set the object up as squeaky. A little bit of logic to swim through here as we look for things that modify chewExperience. Looks like it's only happening if chewExperience isn't squeaky. Wait a minute, looks like chewExperience is getting assigned to. That assignment sets it to squeaky, so that's not the problem with our first test, but it could be the problem for our second test. If you remember, our second test returned squeaky, and it wasn't supposed to. Let's take a look at the constructor, and here's the problem. Looks like our constructor is setting the experience to fair if our object isn't chewy, so we failed a couple of potential problems, but notice these problems are not in the subject we're testing. These problems are in the dogObject code, and what we're actually creating here is not a unit test at all, but an integration test. Integration tests are important, but that's not what we're trying to do. We're trying to write unit tests. This is why our other unit tests are passing. They're isolated from the defects that exist in the dogObject, so we know DogController is working fine, and that's what we're trying to find out. So if you're writing unit tests, isolate your subject under test. If there's a dependency on an object with constructor logic or really any logic you should just mock it.

Summary
So now you know how to mock in Jasmine. Jasmine does a good job of making it easy to leverage their spies to create the isolation you need for your unit tests. In this module we learned about the importance of isolation. We created our first dynamic mocks and stubs, and saw how easy it can be to create a simple spy. We saw another method for creating spies that allowed us to avoid instantiating the mocked object, and thus, not invoking constructor logic. We saw strategies for returning values from a spy that allowed us to duplicate more complex interactions, and we saw what can happen when we think we're running unit tests, and we're actually running integration tests. Next, we'll do some mocking with RESTful services, and we'll talk about how to write tough tests that are resistant to change.

How to Write Tough Tests with Jasmine
Overview
Welcome back to testing JavaScript with Jasmine and TypeScript. I'm Tony Curtis, and yes, I'm alive. In this module we'll discuss the importance of tough tests, and how you can make your tests resistant to non-relevant changes in the application. We'll start our discussion by defining what a tough test looks like, and some strategies for ensuring that you don't end up with egg on your face when you commit the suite of tests. Next, we'll go over the service that fetches awesome photos of Mars from rovers, and we'll do an analysis of exactly how it works. We'll set up a foundation of configuration values, and test variables to make our testing pliable and accurate. We'll examine the important aspects of the interactions for our test subject and what we should be ignoring. We'll introduce some techniques to communicate our apathy. We'll fiddle a bit with the configuration, and see how our tough tests are made to roll with the changes, and finally, we'll look at what happens when tests have broken bad.

Make 'Em Tough
When you create unit tests you want to make them tough. In fact, it would be awesome if you could make your unit tests so tough they're kind of intimidating, like Wolverine of the X-men. I mean, nobody's going to go up to that guy and tell him his hair looks a little strange, but seriously, you really want your test to be tough. What do I mean by tough? A tough test is reliable. If it reports a failure you can count on it being a problem in the app. It's flexible when it needs to be. It bends, but won't break with incidental changes. It's focused. It knows what it's testing and only tests that. It's relevant. It does its job by reporting meaningful issues when they exist. Tough tests are much easier to work with. If you're trying to convince management to dive into unit testing you need tough tests. Whatever you do, don't do this. Fragile tests are just the opposite, unreliable, inflexible, scatterbrained, and irrelevant. You have to handle them with kid gloves. Every time you turn around the break, and you've got to fix them. It's not uncommon to change the app, and have a suite of fragile unit tests fail at a high rate. You see red and failure all over, and you just groan. It's also not uncommon to see these unit tests set to pending, commented out or deleted. So how do we write tough unit tests? The ends justify the means. Don't get wrapped up in the details. Test what not how. Don't micromanage your test subject. Less is more. Simpler tests have fewer points of potential failure. Remember the ridiculous complexity of the Mousetrap game? Lots of points of failure there. Isolate and focus. Test one thing at a time, and test it well. As you design your unit tests ask yourself these questions. What does my test subject really need to do? Notice the emphasis on need. What important changes happen as a result of running my test subject? Are some of these changes incidental or not important? What is my test subject interacting with, and which of those interactions are necessary, and think about what might change in the future. That's a tough question to ask, but give it some thought, and also ask yourself, if this changed, should my test care? Let's touch on some tough strategies for unit testing. Be liberal with your use of test variables. Avoid using literals as part of tested or expected values. They become a potential point of failure. Use configuration values in your testing where possible. Use the values without changing them, then change the values to test the response to configuration changes. Use values that communicate when they're used as placeholders. I'll use a negative number or a silly word to indicate what is trivial, and finally, as I mentioned before, use S. O. L. I. D. coding principles to write excellent code for your tests, particularly by nesting set up values for groupings of tests. You'll see all of these principles used in this module. Mocking allows us to toughen our tests by isolating away complex dependencies. This is especially important when dealing with external components like web services maintained by a third party. Mocking these kinds of dependencies is crucial for making tough tests, but how do we make our mocks tough? Here are some tips. Generally, you'll want to stub to fill the gaps, not a mock to test interactions. Test the end result. Only test interactions that are crucial to the final result. If you must test an interaction test what's important about the interaction. Jasmine makes it possible to only check certain parameters or just check that the call was made, not the details. When testing with mocks it's easy to forget that the values they return are just something you made up. Don't test the values directly, only test their effect on results. For example, testing for a validate from something that originated from a mock lets you know your skill, like creating a mock date. It's kind of like a dog chasing its tail. It's fun to watch, but not very productive. Now let's talk about the test subject for this module.

Dogs in Space
My dog Zeus heard about this Mars rover project and his ears perked right up. He decided to have the Mars rover on his blog site, so he could check every day to see if there's dog life on Mars. Let's take a look. I like what my blog does right now, but it could do so much more. Right now it starts with a default photo from a configured camera for a specific date in the past, since there is a time delay for the photos. It's only getting photos from one particular rover, Curiosity. Zeus chose Curiosity because, well, you know what curiosity did to the cat. We can change the camera to see different photos, and that's great. In the future I'd like to choose the date, and the rover, and browse multiple photos from the camera. I started a refactor for those features, but it's not fully implemented. Let's look at the rover photo data service. Since the service is promise based you'll see promise related code. We're injecting an HTTP service, a couple of configuration objects, a translation service, a validation service, and the promise service. Those are our dependencies. Let's glance at the logic. The first thing you can see is this getTranslatedCameras call. It takes an earthDate parameter. The parameter's optional, and defaults to a config value. Next, a validator's called to assemble the parameters. If these parameters aren't valid the parameter object is returned with a config based rejection value. That value is immediately returned as a broken promise. The validator handles all the validation. The reasons for an invalid date are a malformed date or an out of range date that's not between the rovers first and last dates for photos. If the date passes validation, an additional parameter, the API key from a config setting, is added by the validator. If a valid parameter object is received it's inserted into a request config, which is passed to the HTTP service. Also notice that a config value provides the URL. The response from the call could be a server error returning a 500 series status or a request error, like a bad API key or a badly formatted request, returning a 400 series status. These would be returned as a broken promise. We could also get data. That data may contain photos, an empty array of photos or a JSON object with an error member and the value, no photos found. If we get the normal JSON construct with an array that's empty or full of photos we'll send it to the translator to format the return data, which will return as a successful response. If we get the JSON object that says, no photos found we return a broken promise with a config based special status. Next, we'll do some analysis of our test subject.

Analyzing the Photo Service
First, let's look at our inputs and outputs. We have several dependencies, including dependencies on configuration values, a validator, a translator, an HTTP service, and a promise service. We provide a single date parameter, which can also be empty, causing it to default to a config based date, so possible date values are empty, a valid date that's in the range and has photos, a valid date that's in-range, but has no photos, a valid date that's out of range, and a malformed date. Let's take a look at our outputs. We have no change in state as a result of this call. We make a call to a RESTful service. We return a promise that may contain an error from the RESTful service with the return status, an error with the config status indicating a bad date parameter, an error with a config status for a return of no photos found, and finally, an array of cameras that's either empty or contains cameras. Let's look at the interactions. We rely on our config and constant values with a consumption based interaction. Pretty simple. We must here to a third party spec for the RESTful call, so parameter validation is important, however, the work of validation is delegated to the validator. We can't unit test validation by testing the data service, but this interaction is important, so we'll test the important parts of this interaction. The call to the HTTP service is central to what we're doing. We should know that we're making the correct call, but the confirmation of the query string has been delegated to the validator, so we can only check that the configured URL is used. The data received from the HTTP service is passed to the translator to be reformatted into an array of cameras. The service is responsible for delivering formatted data, so we do care about formatted results, but not about the details, so let's think about what we're testing. We need to know that the validator is called. We need to know that it gets passed a date parameter, either the one passed in or the date from config. We need to see how the test subject reacts to the parameters return from the validator. We don't care about the specific values from the validator, we only care about their effect on logic. For example, we can't really test the difference between a valid out of range date and a malformed date since both come back from the validator as invalid. We need to know the RESTful HTTP service is called when we have valid parameters, and not called when we don't. We need to know if the parameters coming from the validator are passed to the RESTful service. We need to know that our config URL is passed to the RESTful service. We need to know that our test subject responds to whatever response comes from the RESTful service. We need to know that we're getting properly formatted errors and status values matching the config values. We need to know that we're calling the translator, but we're not particularly interested in the details of that call. We want some flexibility there, and we want to know that we get properly formatted cameras back from our call, so now that we know what we're testing we'll write the tests.

Creating a Tough Foundation
So let's create our spec. Start out by describing the file that we're testing. Then we set up our system under test variable. Next, we set up a spy variable for our validation service, and a spy variable for the translation service. Next, we'll create a variable from our Angular HTTP backend service. This is what we'll use to intercept all calls going to the RESTful service. We'll also create a variable for this Angular mock request handler. That'll work in conjunction with Angular's mock of the backend service. We'll go into more details about these down below. Now we'll bring in our two config values, AppValues and RoverConfig. In one of our test groups we'll test these config values as they are. In another, we'll put our own values into these objects to test the applications response to configuration changes. RootScope will help us digest any promises returned from our service. Now we're going to define some test variables to help us with some of the return values from mocks and with expectations. For example, this validated date parameter object will act as a return value from our validation spy. In the beforeEach we hook up our two spies with the calls that we'll use in the tests. Notice that we're spying on two methods for both the translation service and the validation service, and that's it for this beforeEach. These will be used for testing different methods in our service, and for testing with default and custom config values. We'll start by testing our methods with the default config values. We'll use some additional variables for testing our standard configuration. These validated parameters are additional objects that can be returned from our validator. The parameter objects vary according to how many different parameters are being passed to the RESTful service. We also want to define some static values. This will help us build our tough tests. As we use the variables for our spies and expectations in the same test we eliminate the potential failure points we would create if we used literals. In this beforeEach we're going to do a bunch of set up. Creating an instance of the service is different than creating an instance of the controller. If you remember for controller we grabbed the controller service, and passed some parameters, along with our controller name to get an instance. For a service you include a ProvideService in the mock module call, and then use that to set up your values for injection. We're going to inject the translate and validation service spies. We won't inject anything else. Remember that we're going to use the default configuration values, and we don't need to mock the promise service. Angular actually provides its own mocking for the httpBackend. We use the familiar inject service to get instances from Angular. We'll grab the httpBackend mock that we'll use down below. We'll grab our rootScope instance, and we'll grab an instance of our system under test. Now we'll take a look at our configuration values.

Leveraging Configuration
In the other tests I've written I cheated a little by exposing some config values, the event names, to the namespace. Let's take a look at the constants file. Since we want to be good dependency injectors I decided to refactor, so they're no longer available to the namespace. I removed the export statement from the let declaration. Now you can see the only thing exported is the type, not the value. Perfect. For the photo tests we're consuming roverConfig and appValues from this file. RoverConfig holds some values for calling the RESTful API, like the name of query string parameters, and the apiKey used for site access. Note that this is a public key available to anyone, but you can get your own key by requesting it from NASA, allowing you to download a whole lot more photos per hour and per day. RoverConfig also has a collection of rovers. Each rover has a name, its own URL, a default camera, a list of cameras, a minPhotoDate, which is the earliest date that photos were taken, and some other values. AppValues just contains some numbers for return status values to identify some custom errors for calls made to the RESTful API, but how do we get our real values for roverConfig and app values since they're no longer available to the namespace? I refactored the masterControllerSpec to go and get these config values for the test. It's a simple refactor, and here's what it looks like. So you can see, we just use another call to an angular. mock. module to get an injector into app. core. Then we grab eventNames from app. core and set it to a variable. Pretty simple. In our rover photo spec we'll just grab those values from the app core module, since the service we're testing is in that module. A little later we'll actually create our own config values, and pass them in using a provide closure like the one above. So you can see Angular lets us do both. It lets us pull things out of the module using inject, and it lets us provide custom things to pass into the constructor, like we're doing here. So we can run tests with the config values as they are, and we can run additional tests with custom configuration values.

Finishing the Setup
Now we're going to set up some of our variables for testing. We'll set up three different flavors of the validated date parameter. This matches with three different parameter objects returned from the validator with getting two, three or all four parameters. The getTranslatedCameras call only uses the two parameter object, but the other methods can use up to four. Notice too, as we set these up, that we use values from our config objects. That's the theme of this block of tests. Use actual config values, and then we set up our validation spy to return the validated date parameters object. We set this up for two different calls to the spy with a reminder that we can change the return value as needed. Then we're going to use Angular mocks httpBackend to set up something called a backend definition. We do this using the whenGET function. A backend definition is simply a setup that looks for an HTTP call and supplies a response to that call without forwarding it to the actual URL. There are many ways to use this Angular backend, and it's beyond the scope of our discussion to get too deep into that, so what we'll do is set it up to catch anything using a regular expression. The whenGET returns a RequestHandler that allows us to set up a response for the HTTP request. The RequestHandler lets us redefine what the response is without having to define a new backend definition, so it's very flexible. Now we'll go grab some photo data from the roverTestData. What's roverTestData? Let's take a look. What I did is copy some return values I got when I called the rover RESTful service. I used those values to create this test data. It's an easy way to set up some data for testing. Then in our SpecRunner file we just reference the test data prior to using it in your specification. So now all the calls to HTTP will return this photo data without actually calling the service. If you had calls going to different HTTP destinations you can also set up different call patterns to return different data, but that's not our case. This is a simple example. So that's our beforeEach. Pretty involved, but a lot that we don't have to do in each test. A lot of repetitive code saved. Now let's write the specs for the first method in the photo service.

What We Care About and What We Don't
So we set our describe string to the name of the method we're testing. For these first tests we don't need any new variables, and we don't need any set up, so no beforeEach is necessary. For this test we'll pass in an earthDate and expect it to be passed to the validator. We can make the call using a test variable we already set up. Since our validator is set up by default to return valid parameters this call is actually going to go through to the httpBackend, so we need to flush it. What happens with our httpBackend is the response to the call is a promise waiting to be triggered. This simulates the asynchronous nature of an HTTP call. The call to flush tells the backend that the call has been completed, and the promise can be delivered. Our expectation is that we call our validator with the same parameter that we passed to our method under test. Notice how we're using the same variable for the parameter and the expected value. This is one of our tough testing practices, using variables instead of literals. In the second test we'll call the method without sending a parameter. This should result in a default date from a config object being passed to the validator. Our expectation is the call was made to the validator with our configuration value as the date parameter. This is another one of those tough testing principles. Use configuration values. Next, we'll run some tests on the effect of the validators return value on method logic. To do this we'll need to set up some new test variables, and override the setup from above, so we can return invalid parameters from our validator. We'll need some errorData in the form of a broken promise, and a variable to confirm whether or not the HTTP call was made. In the beforeEach we make sure our Boolean is set to false. Then we set up our validation spy to return an object with the error information about the parameter and values that were naughty. Again, we use a variable here that we'll use below in the expectation. We reset our request handler to respond with something we just don't care about. Notice how that's clear in the way this is defined. For these tests we're going to call our test subject in the beforeEach, like we've done on other tests. For the date parameter we pass in a value that we don't care about, but wait you say, we're sending in an invalid date parameter. Isn't that exactly what you want? Remember, we've already set up the validator spy to return an error. Also remember, our validator is not a validator. It's a dumb object that has no logic, and could care less what the heck we're sending it. In fact, in a future test that's looking for a valid return from the validator you'll see I sent it meh. The point is, if we're not testing what the validator is receiving, it doesn't matter what we send it, it's a black hole. Since our call returns a promise, we then call then on that promise, and set it up with two callbacks, one to let us know if HTTP was called, and the other to collect the error data. Finally, we digest to trigger the asynchronous completion of this promise. This is similar to HTTP flash, but the promise isn't going through HTTP. The broken promise is triggered before that call is made. Much smaller set up this time, just overriding some values. Now let's write the tests. Now back to our specifications.

Expecting the Invalid
First, we check to see if we get an error status of -42, which is the value I'm using to indicate that a bad parameter value was sent to my service. Forty two is a special number to me, and negative forty two is just the opposite. Notice again, how we're using a config value for the expectation. Next, we check the error text to make sure it contains the value we returned from the validation spy. We want this info returned since we may want to log it to help us debug any problems. Again, in the expectation we use the same test variable we set up with our spy. If our validator returns an error we shouldn't call the HTTP service, we should bail out, but there's an issue here. If we call back in flush, and there are no pending requests, we'll get an error. There shouldn't be any pending requests, so we don't want to throw an error here. That would cause a perfectly good test to fail for the wrong reason, but we do need to check this. Fortunately, Angular has a bizarre version of flush. It's a verifyNoOutstandingRequest function. This will return an error if there are things in the buffer that need to be flushed. If it returns an error the test will fail. Since there shouldn't be anything in that buffer the test should fail if there is, so this is exactly what we want. We'll finish up by just making sure that HTTP was not called, for grins. There's actually no chance for this expectation to be run if it were true, but since verifyNoOutstandingRequest isn't setting any kind of expectation I need to expect something, so I'll run an expectation on wasHttpCalled. We'll actually talk more about this little quirk in the next module. The next thing that would happen in our chain of activities is when our validator returns valid parameters we would take those parameters and pass them to the HTTP service. Since that service is mocked by Angular's httpBackend we would run an expectation on those parameters, but that's actually what I'm going to cover in the next module, so I'll have to skip those tests until the next module. Are you sick of me talking about the next module yet? Next up, we'll test what should happen when our HTTP service is called returning both valid and invalid responses.

The RESTful Response
So we move on to what happens when our HTTP service returns things. Remember, the things the HTTP service returns are exactly what we tell it because it's a mock. We start out with what happens when it returns an invalid response with a status of 500, which means there's a problem with the server. Again, we're going to want to grab some error data. In our beforeEach we'll set up our RequestHandler to respond with the status error by using a test variable. Notice in the respond call of the RequestHandler, if the first variable is a number the second variable is a string, and it's not optional. That string represents some data returned by the response, and I just set that to an empty string. I could have used meh if I wanted to. Again, we call getTranslatedCameras with something we don't care about. Our validation service will return a valid params object to pass to the HTTP call. We set up the service call to handle the promise with nothing happening on a success, and grabbing the error data on an error. This time we'll be getting a response, so we flush the HTTP service, and we digest the promise. First, we test for a status of 500 using our test variable. Next, we'll see if our translator spy got a call. Remember, our translator is used to take the return data from the HTTP service and translate it into the kind of JSON object that we want to consume, but if the HTTP response isn't valid we shouldn't be calling the service. For the RESTful service we're calling there's another condition that we need to handle. If we make a call, and there aren't any photos for that date, it doesn't return an error. Instead, it returns a JSON object that's formatted differently. It has one field named errors with a string value that says No Photos Found. We've designed our service to handle this condition, so let's test for it. Our service is set up to return an error if we get no photos, so we get our error variable ready. In our beforeEach we set up our RequestHandler to respond with this NoPhotos object using a test variable. We call our system under test, setting up the promise callbacks to do nothing with a valid response, and to grab the error for an invalid response. We flush and digest. That seems counterintuitive. You would think digest then flush, but the order is correct. You flush the HTTP, then run digest to trigger the promise. In our first test we use a configuration value to check for a special return status indicating there are no photos. In the test subject we're using that same config value to send the status. If at some future point someone changes that config value our test wouldn't break. In our second test, again, we shouldn't be calling the translator if there are no photos to translate. For our last tests you'll see what happens when we actually get valid data. If our HTTP service returns a valid response we should make a call to the translator. I'm not going to specify that call. The translator doesn't need all of the data that would be returned from this call. It really only needs one photo. I don't want to tell the HTTP service and the translator how to do their job, so I'm only going to insist that the two are talking to each other. For the final test we actually get some formatted camera data from the translator. Notice we didn't have to load these cameras until the last test. They'll be the mock return from the translator. Now we make our call, and capture the return values. We use toBe because we expect the same camera instances provided by the mock translator to be returned to the promise. Now let's run these tests.

Configuration Mess, Testing Success
So looking at the results, all of the tests are passing. As we take a look at our spec it's easy to read and well organized. Reading through the spec we'll tell you exactly what this method is supposed to do, so while you were looking at those test results I automagically created some specs for our custom config values. Let's take a look. So there's our describe, and here's our RoverConfig setup. You can see I don't care about anything here except the minPhotoDate. In our app config I care about two status values, BadParam and NoPhoto. In provide I set up with a configs in addition to the spies, since I have custom configs to inject. In inject I don't grab configs since I already have them. We set up some test variables and the spy for the validation service. We don't set up a spy for the translation service because we don't need it. We set up the httpBackend, and our first to describe. The first config value we're going to check is our minPhotoData. We test that by calling getTranslatedCameras without a date parameter. We expect it to be called with the configuration value. Next, we're going to test some error return values. We setup some variables for the test. Then in our beforeEach we set our validation service to return invalid parameters. We set up a backend definition with values we don't care about. We called getTranslatedCameras and set up the promised callbacks. Finally, we called digest to make sure the promise is returned. We're testing to see if our error status value matches what our configuration says it should be. In our last test we see what happens when we get the no photos return. We set up our handler, our promise callbacks, then we flush and digest. We expect to get our custom configuration value for the NoPhotos status. Let's run these tests and see what happens. It looks like all our tests pass. What this proves is our application actually responds to changes in configuration values. That means our app is driven by configuration. That's what we want, and that's what we've tested. Now let's see what happens when you don't write tough tests.

The Impact of Fragility
So what happens if we don't write tough tests? Here I have a suite of tests that were written using very few test variables, lots of literals, and no configuration values. As we look at the tests they look more concise, and maybe even simpler. They're some of the same tests we made with our tough test techniques. Let's run these tests and see what they look like. Looks like we have eight failures. Let's see what failed. Looks like just about every test failed. Let's take a look at the failure details and see if we can fix them. Our first test is failing because we got an unexpected request, but what's strange is we're testing the call to the validator, and we really shouldn't care about the request. Looks like it's the same problem for the second test. Let's take a look at the test code. So the problem is we're setting up the backend with an expectation that's super specific. Since we're not testing anything related to the HTTP service we shouldn't really care. Let's make this match anything, and that error will go away. We still have eight failures, but if you look at the failure text you'll see we're failing because of our expectation, not because of the URL. Our first test was expecting January 1 and got January 10. Our second test is looking for a date string with leading 0's, and the date string it got didn't have leading 0's. The fix for this first error is easy, but it wouldn't have surfaced if we had used a test variable in the spy and expectation. To fix the second I'm actually going to change the value in the configuration, since that's where the date comes from when we don't send a date. Now we'll run the tests again. Looks like we fixed our two tests. That's great, but I want you to notice something else. We just went in and changed a configuration value, and none of our tough tests broke. That's because we relied on config values. Now let's go back to our failures and see if we can fix some other tests. Looks like our next failure says it's an unsatisfied GET request, and again, that shouldn't matter for this validator test. Back to our test, it looks like we're using the pattern that accepts any request, so that's not a problem. Let's scroll down and take a look at something. For this test we see a new construct for set up called afterEach. You'll frequently see this used in tests. It's there to verify there are no outstanding requests or expectations. In our backend definition we're using expectGET. When you use expectGET you set up an expectation that gets triggered when you call verifyNoOutstandingExpectations. So if this backend definition doesn't get hit it will throw an error, so when you set up httpBackend you want to use whenGET whenever possible. WhenGET doesn't care if the backend is hit or not. It's just ready to send a response if needed. Only use expectGET when you need to test something about the call itself. Only test what you need to. So we run our tests again, and that fixed three of our failures. Our next failed test has to do with the valid HTTP response. Let's take a look at the details. Okay, so we expected to see supercalifragilisticexpialidocious, and our actual value was supercalifragi. Hmm. Let's look at the test. So this seems like a reasonable test. We expect our translator to be called with the rover test data being returned from the HTTP service. Let's look at the code. Okay, it looks like we're not making this call with all of the returned photos. I mentioned previously, all we need to get cameras is one photo. That's exactly what we're sending here, the first photo in the array. This is why I didn't want to over specify the call to the translator. I really want my service and the translator to work that out between them. We'll loosen the specification by just looking for the call. We run the tests again, and now we're down to two failures. Looking at the next failure, again, we have an unexpected request. Looking at the code, we have over specified the URL. We aren't even testing the URL, so let's get rid of the specification, and just pass the request through. Okay, that fixed that test. We only have one problem left. This one looks a little strange, but I can tell by looking at the test that we have some issues. First, when we send error, no photos found, we should be getting back an error. We're not set up for that, so let's fix it. When we run the test again, and now we see our error has changed. This test was based on the code before I changed it to return an error for no photos, so let's fix that. We look for the status to be -37, and we run the test. We finally got our test passing. Hooray, but it was a bit of work fixing those tests and, looking at our test code, it was hard to tell what we were testing. Every test was sending valid data for all of the calls, so you had to think about what was going on to troubleshoot the problems. You really got no help from the test code. Now let's try something radical. Let's go in and change some config values. Zeus heard about another rover on Mars called Opportunity that's been there for 12 years. He figures that finding dog life on Mars is going to be a lot easier if you can look through 12 years of photos, so I created a configuration for opportunity, and we're going to set that as our defaultRover. We're going to change some of the other configuration values, and save it. We run the tests again, and we get three failures. You can see that our tough tests are doing just fine. They relied on the configuration values, but our brittle tests didn't fare as well. I hope as we've taken a look at some of these testing strategies you can see the value of not over specifying any part of your test, making it clear what's not important in your test, and using test variables and config values to your advantage.

Summary
That completes our module on tough tests. We've seen how to use Jasmine in Angular to create tests that withstand trivial change and still give us meaningful information about our test subject. In this module we learned why tough tests are so important, and how to implement tough strategies in your tests. We learned how to analyze a test subject with dependencies to sift out the important interactions for testing. We learned how to avoid literals in our testing where they would add points of potential failure. We learned to recognize where we might be tempted to test our mock data, so we can avoid those awkward moments in code review. We learned a few tricks about testing promises, and how to manage asynchronous testing in Angular, and we learned the pain of maintaining brittle tests that are difficult to troubleshoot, and often over specify what your test subject is supposed to do. Next, we'll take a look at test signals and how they can either help you, annoy you or lull you into a false sense of security.

False Hopes: Which Is Worse, a False Negative or a False Positive
Overview
Welcome back to testing JavaScript with Jasmine and TypeScript. I am now, and continue to be, Tony Curtis. In this module we'll discuss some testing terminology that seems to be a double negative, and we'll cover some of the pitfalls that you may run into when your tests don't do what you think they're doing. We'll start out by finishing off the tests we didn't complete on our RoverPhotoDataService. We'll show you how to verify that the HTTP service got the URL and parameters it was supposed to get. Next, we'll test the URL using a different technique that mostly works, but ends up doing something strange. We'll talk about what it means to be positive in testing terms, and you'll discover that negativity is a good thing. Finally, we'll show you how to make sure that testing's evil twins never show up at the family reunion.

Testing the Call to the HTTP Service
In this module we're going to start right off in the code. We'll finish up the spec for the RoverPhotoDataService by going back to some tests we skipped. They're tests for what the HTTP service receives when the parameter validator returns valid parameters, so let's finish these. One note, I added some tests for the other methods while you were switching modules. I know that's hard to believe, but it's true. In fact, I added those tests before you switched modules. So now you'll see a lot more tests. Don't panic. Here are some tests for validator errors. Let's get passed to those, and here we test the HTTP response, so let's sandwich our tests in here. Start off by describing our conditions. This time we get valid params from the validator. Below, we'll check to see if we forwarded them to the HTTP service. We're going to snap the URL that we sent to the HTTP call, so we need a variable for that. In the beforeEach we reset the variable. Then we use expectGET, but wait, I told you not to use expectGET. Actually, I said not to use it unless you were testing the URL, and we are. A brief note about Angular strategy with the httpBackend fake whenGET, expectGET, and similar request utilities. WhenGET and its many when cousins, like when POST and others, are there to set up what Angular calls a backend definition. This allows the test to define certain URL patterns, which will be detected for the incoming request, and allows a specific response to be defined for that request. As mentioned previously, the one functions don't care if they're called or not, they're just looking for a matching call, so they can return the defined response. The expectGET and its many expect cousins are there to set up something called a request expectation. These are designed to examine the URL, and throw an exception if it doesn't match a certain pattern, causing a test to fail. These are meant to be strict and will throw an error if they're not called when the verifyNoOutstandingExpectations function is invoked. They're similar to the when functions, just stricter, and can be layered with the when functions to operate on the same request. That is what I'm doing, layering the expectGET on top of the whenGET, so I can use expectGET to grab the URL with whenGET defining the response. If you remember, we defined those responses further up the test scope. The Angular httpBackend has fantastic documentation on these functions, and all of the options with their fake httpBackend service, so check it out. So in our expectGET we have lots of options for parameters, as you can see by the documentation. One option is a function that receives the URL sent to the HTTP service, and returns a Boolean to indicate if the URL matched the expectation. That is what we'll use. We want to grab the URL that was passed to the HTTP service, so we can see if it has what we think it has, so that's what we'll do first, then, since the expectGET is looking for a Boolean in return we'll send true. We just want to grab the URL, not test it here, so we'll just tell the expectGET, hey, no worries, the URL is fine. Then we'll test it below with a Jasmine expectation. Might seem a little strange, but I'll explain it later. Then we'll make the call, and flush that response from the HTTP service, and that's the beforeEach. Next, we'll write the tests.

Writing the Tests and Discovering Keys
Now for the tests. We start with a check of the base URL, which should come from a config setting. We expect the actual URL we grabbed to match our config value, so let's use the toMatch option here. We'll use a regular expression with this. This also gives me a chance to rectify something I left out of the previous module, an introduction to a new key on your keyboard, the back tick. No, this is not some insect that sucks blood from your back, but an awesome new key on the keyboard that will help you to write strings with JavaScript. Okay, it's not a new key, just the seldom used key located right below the escape key. The back tick is used in the ES6 implementation of string interpolation. Ha. How often do you get to say implementation and interpolation in the same sentence? This allows us to insert values into our strings for evaluation, much like the string format functionality in C++, C#, Java, and other languages. Thanks to TypeScript this ES6 feature is available, like other language enhancements we've used, class, let, and others. The syntax uses back ticks to enclose the string in place of single or double quotes. That is combined with the dollar sign curly braces that define template literals. The syntax allows the evaluation of values within the string. In our example here we start with our back tick, start our regular expression with a beginning of line character, then our template literal, so we can reference our config value. After the template literal we finish up with our regular expression tokens for any character repeated any number of times. Then we close with another back tick. Again, notice that I am using a configuration value, and also note we will add this same test to our custom configuration test at the bottom of the suite. The next test will check for the earth_date parameter, which we set up to be passed as a return value from the validator spy. We make sure that parameter made its way to the HTTP service call, using the toMatch matcher. It's funny that there's a toMatch matcher. Next, we do the same with the api_key parameter. Now you might thing, hey, that's a configuration value, why aren't we checking this with the configuration? Well, we aren't getting it from configuration. The validator does, but we aren't testing the validator. We get this value from the validator, so we check it against the validator value. The URL test above is a different case. We do get that from config values, not from the validator. Okay, those tests are done. Now let's head down to the bottom of the spec file to where we're testing the custom config values, and duplicate our URL test there. There's our param validator error test, so our good params validator test will be right after this. Yup, and right before the HTTP return test, perfect. We add our description, set up our actual URL, and beforeEach like above or we snag the URL. Call for translated cameras, and flush. The test description is the same as before, and so is the test. The difference being we have changed the URL to a custom value, so we can ensure that the call is based on config values, and we're done. Let's run these tests, and they pass, so now we know our validated params are being delivered, and that our configuration driven URL is working. As we scroll down you'll see similar tests for the other method calls, and they also pass. At the bottom we test for our custom config values, and that test is passing. Cool, but what was all that about not using expectGET to test our URL? If you look at the Angular docs for httpBackend that is how they tell you to do it, so what gives? We'll take a look at that next.

Setting an Unexpected Expectation
So you want to test this with expectGET eh? Fine. You wanted to go there, let's go there. We'll just add one test outside of our describe scope, and we'll set our expectation string to something similar to what we have above. We'll use the expectGET, but this time we use a simple, regular expression to make the match. We don't need a function because we're not grabbing the URL. We make our call, and flush the back end. That's kind of funny. I swear, these framework guys have a sense of humor. Now we want to ensure the expectation is fired, so we use verifyNoOutstandingExpectation. We finish off with verifyNoOutstandingRequest for good measure. Note that if we set this up as a describe scope we would put these last four lines in an afterEach, and the tests would only contain the expectGET expression to match each parameter and URL. Let's run the tests. Everything passes, so what were you worried about? Wait a minute, spec has no expectations. Hmm. I used expectGET. I used a verifyNoOutstandingExpectations. What else do you want from me? Most testing frameworks would be fine with this test, even if no native expectation or assertion is ever executed. They will also fail if any error is thrown for any reason, and Jasmine does that. Angular was exploiting this feature with their expectGET implementation. It will throw an error if the URL does not match, and will not throw an error if it does. In Jasmine 2. 0 and earlier that worked like a charm, no exception thrown, so the test passes, and nothing weird going on here. At the time the documentation was written the test worked just fine, but in version 2. 0. 1 Jasmine added a safety net to check for whether you used a Jasmine expect statement in your test. Now if you don't include a Jasmine expect call it will warn you with a big ugly shout, spec has no expectations. It doesn't fail the test, but it uglifies the reported spec. Realize Jasmine has no way of knowing that you have an Angular expectGET in your tests. It knows nothing about Angular at all, so it doesn't recognize the expectGET as an expectation. If you look at our test you can see we have no Jasmine expectation, but why on earth would Jasmine do this, and ruin such a beautiful thing? To prevent a false negative. So what's a false negative?

Testing's Evil Twins: False Positive and False Negative
Let's talk about positive and negative test results, and their evil twins, the false positive and false negative, and let's talk about smoke detectors. So we go buy a smoke detector and some batteries, and we bring it home. We install the battery in the detector, and thankfully the alarm doesn't go off. That's because there is no smoke when we're putting the battery in, unless for some reason we decided to install the battery after the fire had already started. The fact that the alarm didn't go off after we installed the battery is known as a negative test signal. This is kind of counterintuitive because we think, well if the alarm didn't go off that's a positive. That means things are going great. There's no smoke, there's no fire, seems pretty positive to me, but what a negative test result means is that there is no signal, and that's what we want when there's no smoke, so just remember, a negative test result means there's no signal. Think, I tested negative for strep. So, we climb up the ladder, and mount the thing on the ceiling. Now the days pass, and we almost forget it's there. We change the batteries every daylight savings time, and it gives us some piece of mind that we have some protection if there ever was a fire. Then one night we hear the smoke alarm go off. We wake up with a start and we smell smoke. We trace the smoke back to that piece of greasy piece of pizza that we accidentally left in the toaster oven when we were thinking about a midnight stack. Looks like the grease caught fire, and our toaster oven is toast. We grab the fire extinguisher and put the fire out. The alarm going off is known as a positive signal. Again, it's counterintuitive to think of a fire in your house as anything positive, but what it means is, a signal was thrown. The smoke detector detected some smoke, and threw the signal, so remember, the positive test result means there was a signal. Think, I tested positive for strep. Now we're really glad we bought that smoke detector, so a week later we're sound asleep, and the alarm goes off again. We crawl out of bed, but this time we don't smell the smoke, so we go to where the smoke alarm is, and we don't see any smoke. We look around, and we sniff the air. No sign of smoke. We pull the shrieking smoke alarm down and knock it a couple of times to see if we can get it unstuck. Finally, we just take the battery out. Our ears are ringing, we're groggy, and we're really mad. We go back to bed, and try to get back to sleep with the thought that we need to go get a new smoke detector in the morning. What we've just experienced is known as a false positive. This means a signal was thrown, but there was no problem. We'll discuss the ramifications of this shortly. Think, I tested positive for strep, but I didn't actually have strep. The next morning we go to the store and get a new smoke detector. Again, we put the battery in, mount it on the ceiling, climb down from the ladder, and hope this darn smoke alarm doesn't wake us up in the middle of the night again. A few months later, after the smoke alarm has proven itself to be reliable, and hasn't gone off for some random reason, we decide to have a midnight snack again, using our brand new toaster oven, and an even more greasy piece of pizza. After we put the pizza in the toaster oven and start it cooking we fall asleep on the couch. Once again, the pizza catches fire and engulfs our toaster oven, putting off large amounts of smoke. However, when we installed the new smoke alarm we used an old battery. The toaster oven burns, the fire spreads, eventually we wake from our slumber to the smell of a lot of smoke, and we only have time to get ourselves out of the house to safety. Now the house is toast. What we've just experienced is known as a false negative. This means there was a problem, but no signal was thrown. The ramifications of this can be huge. Think, I tested negative for strep, but I seem to be getting worse by the day.

Keeping the Twins Away By Testing for a Positive Result
Now we will apply the example to test results in general. A negative test result means no signal is thrown. In a stable environment this should be the normal state. When all is going well you're just cruising along, and hardly even realize you have any tests or alarms or anything like that. All is nice and quiet. The outcome of this is that we feel a sense of security. We feel assurance that nothing is wrong. We're free to focus on other things. For unit test this happens when we write a test, run the test, and it passes. A positive test result means a signal is thrown. In a faulted environment this gives us an indication that something is wrong, and that's exactly why we have the test or the alarm or whatever it is, so that we know that we need to take some action. The outcome of this is that we take some action. We have to do something because the system will either annoy us like a shrieking alarm or it might prevent us from getting something done, like when a build fails. For unit test this happens when we create and run a test or run a test on changed code and it fails, pointing out a flaw in the code. A false positive test result means the signal is thrown, but there is no problem. For a stable environment this is telling us we need to fix something, but there's nothing to fix. There's actually nothing we can do. The outcome of this is we'll have to take some action because either the build won't build or the alarm won't stop shrieking, so we pull the battery out of the smoke alarm. For unit tests this happens when a test fails, and there's no problem with the code. We either fix the test if we have time or we ignore it, comment it out or delete it. The side effects of false positives are a tendency to grow numb to positive signals. If we have a lot of tests that are frequently throwing false positives we start assuming that everything is a false positive rather than investigating to see if the failure is in the code. A friend of mine had a car whose engine light would constantly go off, yet the mechanic would never find any issues with the vehicle. In the end she put a piece of duct tape over the light. To prevent false positives we write tough tests, tests that only fail for the right reasons. A false negative test result means there's a problem, but no signal is thrown. For a faulted environment this means there's a real problem, but nothing triggers a response to the problem. The outcome of this is that no action is taken. The issue remains and often gets worse, like the firs in our toaster oven. For unit tests, this happens when all the tests pass, but the code has problems that we thought were covered by the tests. Unless some other signal indicates a problem it will go unnoticed until finally, it's discovered by QA or a user. A side effect of the false negative is distrust of the testing system. It becomes irrelevant, and additional manual testing is required to ensure product quality. To prevent false negatives we can either use TDD or we can make testing for a positive signal part of our test creation process. Think of pressing that little test button on the smoke detector. That makes the alarm shriek. We need to do that with our tests. TDD or Test Driven Development requires us to write a test first before the code is written, run the test, and watch it fail. Then we write the code to fulfill the test specification, rerun the test, and watch it pass. Confirming the positive signal is part of this process, so this is a very good process for creating unit tests. If you are not using TDD, then one way to test for a positive signal is to short circuit the method under test. This can be done by temporarily placing a return value at the beginning of the method. This should make all of your tests fail, since nothing is happening in the method. Then remove the line, and run the tests again. So when Jasmine changed their framework to ensure that a test has an expectation they were trying to warn you about a potential false negative. Hey buddy, you've written a test here with no expectation. I don't think it's testing what you think it's testing. I have actually seen tests that accidentally didn't have an expectation or, because there was an if condition in the test, didn't run an expectation. Those created tests that were reported as passing for testing some feature, but they were actually testing nothing. So, in my opinion, hooray for Jasmine for recognizing this potential for creating false negatives in our test suites. False negatives are much less common, but can be far more impactful on our system than false positives. I'm glad we have a test framework that warns us about false negatives. Nice work Jasmine. So back to our original test. Let's short circuit the getTranslatedCameras method and see what happens. We just insert a return at the top of the method. Now the TypeScript will complain about unreachable code if we don't throw some logic in there, so let's just do that, and now we have our short circuit, and we run the tests. All the translated camera tests fail, and we're getting a positive signal. This is not the best way to verify a positive signal in every case, just one way to do it. The bleeding redness is your reminder to remove that line of code. In your testing process you can write a line that causes failure, run the test, then remove the line and run the test. Just make sure you remove the line, but in most cases, the line will cause blatant failure of tests, so it should be pretty obvious.

Module Summary
This completes our course on Testing JavaScript with Jasmine and TypeScript. I hope you enjoyed it. We talked about how to get up and running with TypeScript, Jasmine, and introduced you to the demo project, the virtual dog. We discussed how to optimize your testing strategy by keeping it simple, and making sure your tests communicate how and what they're testing. We introduced Jasmine mocking, and talked about the importance of isolation in testing. We learned how to mock the Angular HTTP service, and how to focus on testing the right things in complex interactions, and in this final module we learned the ins and outs of positive and negative test results, and how to avoid the pitfalls of false signals. Finally, I want to thank you for taking the time to view this course. I know your time is valuable, and my hope is that this course will help you to write excellent, meaningful tests for your code.